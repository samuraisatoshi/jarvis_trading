# Reinforcement Learning Configuration

algorithm:
  primary: PPO
  secondary: SAC
  ensemble_voting: majority

training:
  total_timesteps: 100000
  episodes: 100
  learning_rate: 0.0003
  gamma: 0.99  # Discount factor
  gae_lambda: 0.95  # Generalized Advantage Estimation
  batch_size: 64
  n_steps: 2048

ppo_config:
  n_epochs: 10
  clip_range: 0.2
  ent_coef: 0.0  # Entropy coefficient

sac_config:
  learning_starts: 100
  target_update_interval: 1
  tau: 0.005  # Polyak averaging

optimization:
  algorithm: optuna
  n_trials: 100
  timeout_seconds: 3600

hyperparameters:
  learning_rate: [1e-5, 1e-3]
  gamma: [0.95, 0.999]
  batch_size: [32, 128]

backtesting:
  years: 2
  fee: 0.001
  slippage: 0.0005
